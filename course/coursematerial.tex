\documentclass[12pt,a4paper	,twoside]{article}

% ------------ packages -------------

\usepackage[utf8]{inputenc}
\usepackage[OT1]{fontenc}
\usepackage{graphicx}
\usepackage[english]{babel}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{todonotes}
\usepackage{etoolbox}
\usepackage{url}
\usepackage{enumerate}
%\usepackage{tikz}
%\usetikzlibrary{shapes.misc,fit}

\usepackage[backend=bibtex, style=authoryear, maxbibnames=4, minbibnames=4, %
            block=space, backref=false, dashed=false, firstinits=true, sorting=nyt]{biblatex}
\addbibresource{../bib/lundrefs.bib}

\usepackage[bookmarks]{hyperref}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}
\newcommand{\posrealszero}{\reals_{\ge 0}}
\newcommand{\naturals}{\mathbb{N}}

\newcommand{\dd}{\,\mathrm{d}}

\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\renewcommand{\vec}[1]{{\bs#1}}

\newcommand{\uz}{^{(0)}} % upper zero
\newcommand{\un}{^{(n)}} % upper n
\newcommand{\ui}{^{(i)}} % upper i

\newcommand{\ul}[1]{\underline{#1}}
\newcommand{\ol}[1]{\overline{#1}}

\newcommand{\E}{\operatorname{E}}
\newcommand{\V}{\operatorname{Var}}

\newcommand{\ber}{\operatorname{Bernoulli}} 
\newcommand{\bin}{\operatorname{Binomial}}
\newcommand{\be}{\operatorname{Beta}} 
\newcommand{\bebin}{\operatorname{Beta-binomial}}
\newcommand{\norm}{\operatorname{N}}

\newcommand{\logit}{\operatorname{logit}} % logit

\newcommand{\code}[1]{\emph{\ttfamily #1}}

\newtheorem{myex}{Exercise}

\input{nydefs.tex}

\title{Generalized Bayesian Inference\\ with Sets of Conjugate Priors}
\author{Gero Walter}
\date{Lund University, 15.12.2015}

\begin{document}
\maketitle

\section{Bayesian basics}

Bayesian inference allows to combine information from data
and information extraneous to data (e.g., expert information)
into a `complete picture'.
Data $\vec{x}$ is assumed to be generated from a certain parametric distribution family,
and information about unknown parameters is then expressed by a so-called prior distribution,
a distribution over the parameter(s) of the data generating function.

As running example, let us consider an experiment with two possible outcomes,
success and failure. The number of successes $s$ in a series of $n$ independent trials
has then a Binomial distribution with parameters $p$ and $n$,
where $n$ is known but $p \in [0,1]$ is unknown.
In short, $S\mid p \sim \bin(n,p)$, which means
\begin{align}
f(s\mid p) &= P(S = s \mid p) = {n \choose s} p^s (1-p)^{n-s},\quad s \in \{0, 1, \ldots, n\}\,.
\label{eq:binompmf}
\end{align}
Information about unknown parameters (here, $p$) is then expressed
by a so-called prior distribution, some distribution with some pdf, here $f(p)$.

The `complete picture' is then the so-called posterior distribution,
here with pdf $f(p\mid s)$, expressing the state of knowledge after having seen the data.
It encompasses information from the prior $f(p)$ and data
and is obtained via Bayes' Rule,
\begin{align}
f(p \mid s) &= \frac{f(s\mid p) f(p)}{\int f(s\mid p) f(p) \dd p}
             = \frac{f(s\mid p) f(p)}{f(s)} \propto f(s\mid p) f(p)\,,
\label{eq:bayesrule}
\end{align}
where $f(s)$ is the so-called marginal distribution of the data $S$.

In general, the posterior distribution is hard to obtain,
especially due to the integral in the denominator.
The posterior can be approximated with numerical methods,
like the Laplace approximation or simulation methods like MCMC (Markov chain Monte Carlo).
There is a large literature dealing with computations of posteriors,
and software like BUGS or JAGS has been developed
which simplifies the creation of a sampler to approximate a posterior.


\section{A conjugate prior}

However, Bayesian inference not necessarily entails complex calculations and simulation methods.
With a clever choice of parametric family for the prior distribution,
the posterior distribution belongs to the same parametric family as the prior,
just with updated parameters.
Such prior distributions are called \emph{conjugate} priors.
Basically, with conjugate priors one trades flexibility for tractability:
The parametric family restricts the form of the prior pdf,
but with the advantage of much easier computations.%
\footnote{In fact, practical Bayesian inference was mostly restricted to conjugate priors before the advent of MCMC.}

The conjugate prior for the Binomial distribution is the Beta distribution,
which is usually parametrised with parameters $\alpha$ and $\beta$.
\begin{align}
f(p\mid\alpha,\beta) &= \frac{1}{B(\alpha,\beta)}\, p^{\alpha-1}\, (1-p)^{\beta-1}\,,
\label{eq:betadensab}
\end{align}
where $B(\cdot,\cdot)$ is the Beta function.%
\footnote{The Beta function is defined as $B(a,b) = \int_0^1 t^{a-1} (1-t)^{b-1} \dd t$
and gives the inverse normalisation constant for the Beta distribution.
It is related to the Gamma function through $B(a,b) = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$.
We will not need to work with Beta functions here.}
In short, we write $p \sim \be(\alpha,\beta)$.

From now on, we will denote prior parameter values by an upper index~${}\uz$,
and updated, posterior parameter values by an upper index~${}\un$.
With this notational convention,
let $S\mid p \sim \bin(n,p)$ and $p \sim \be(\az,\bz)$.
Then it holds that $p \mid s \sim \be(\an,\bn)$,
where $\an$ and $\bn$ are updated, posterior parameters, obtained as
\begin{align}
\an &= \az + s\,, & \bn &= \bz + n - s \,.
\label{eq:betapostab}
\end{align}
From this we can see that $\az$ and $\bz$ can be interpreted as pseudocounts,
forming a hypothetical sample with $\az$ sucesses and $\bz$ failures.
\begin{myex}
Confirm \eqref{eq:betapostab}, i.e.,
show that, when $S\mid p \sim \bin(n,p)$ and $p \sim \be(\az,\bz)$,
the density of the posterior distribution for $p$ is of the form \eqref{eq:betadensab}
but with updated parameters.
(Hint: use the last expression in \eqref{eq:bayesrule}
and consider only the terms related to $p$ for the posterior.)
\end{myex}

You have seen in the talk that we considered a different parametrisation of the Beta distribution
in terms of $\nz$ and $\yz$, obtained as
\begin{align}
\nz &= \az + \bz\,, & \yz &= \frac{\az}{\az+\bz}\,,
\label{eq:betareparam}
\end{align}
such that writing $p \sim \be(\nz,\yz)$ corresponds to
\begin{align}
f(p\mid\nz,\yz) &= \frac{p^{\nz\yz-1}\, (1-p)^{\nz(1-\yz)-1}}{B(\nz\yz,\nz(1-\yz))}\,.
\label{eq:betadensny}
\end{align}
In this parametrisation, the updated, posterior parameters are given by
\begin{align}
\nn &= \nz + n\,, &
\yn &= \frac{\nz}{\nz + n} \cdot \yz + \frac{n}{\nz + n} \cdot \frac{s}{n}\,,
\label{eq:betapostny}
\end{align}
and we write $p\mid s \sim \be(\nn,\yn)$.

\begin{myex}
Confirm the equations for updating $\nz$ to $\nn$ and $\yz$ to $\yn$.
(Hint: Find expressions for $\az$ and $\bz$ in terms of $\nz$ and $\yz$,
then use \eqref{eq:betapostab} and solve for $\nn$ and $\yn$.)
\end{myex}

From the properties of the Beta distribution,
it follows that $\yz = \frac{\az}{\az+\bz} = \E[p]$
is the prior expectation for the success probability $p$,
and that the higher $\nz$, the more probability weight will be concentrated around $\yz$,
as $\V(p) = \frac{\yz (1-\yz)}{\nz + 1}$.
From the interpretation of $\alpha$ and $\beta$ and \eqref{eq:betareparam},
we see that $\nz$ can also be interpreted as a (total) pseudocount or prior strength.

\begin{myex}\label{ex:dbetany}
Write a function \code{dbetany(x,n,y, ...)}
that returns the value of the Beta density function at \code{x}
for parameters $\nz$ and $\yz$ instead of \code{shape1} ($=\alpha$) and \code{shape2} ($=\beta$)
as in \code{dbeta(x, shape1, shape2, ...)}.
Use your function to plot the Beta pdf for different values of $\nz$ and $\yz$
to see how the pdf changes according to the parameter values.
\end{myex}

The formula for $\yn$ in \eqref{eq:betapostny} is not written in the most compact form
in order to emphasize that $\yn$, the posterior expectation of $p$,
is a weighted average of the prior expectation $\yz$ and $s/n$ (the fraction of successes in the data),
with the weights $\nz$ and $n$, respectively.
We see that $\nz$ plays the same role for the prior mean $\yz$
as the sample size $n$ for the observed mean $s/n$,
reinforcing the interpretation as pseudocount.
Indeed, the higher $\nz$, the higher the weight for $\yz$
in the weighted average calculation of $\yn$,
so $\nz$ gives the strength of the prior as compared to the sample size $n$.

\begin{myex}
Give a ceteris paribus analysis for $\E[p\mid s] = \yn$ and $\V(p\mid s) = \frac{\yn (1-\yn)}{\nn + 1}$
(i.e, discuss how $\E[p\mid x]$ and $\V(p\mid s)$ behave) when
\begin{enumerate}[(i)]
\item $\nz \to 0$,
\item $\nz \to \infty$, and
\item $n\to \infty$ when $s/n = \text{const}$.
\end{enumerate}
and consider also the form of $f(p\mid s)$ based on $\E[p\mid s]$ and $\V(p\mid s)$.
\end{myex}


\section{Conjugate priors for canonical exponential families}

Fortunately it is not necessary to search or guess to find a conjugate prior to a certain data distribution,
as there is a general result on how to construct conjugate priors
when the sample distribution belongs to a so-called \emph{canonical exponential family}
\parencite[e.g.,][pp.~202 and 272f]{2000:bernardosmith}. %Def.~4.12 and Prop.~5.6
This result covers many sample distributions, %relevant in a statistician's everyday life,
like Normal and Multinomial models, Poisson models, or Exponential and Gamma models,
and gives a common structure to all conjugate priors constructed in this way.

For the construction, we will consider distributions of i.i.d.\ samples $\vec{x} = (x_1,\ldots,x_n)$ of size $n$ directly.%
\footnote{\label{foot:dataisplural}It would be possible, and indeed is often done in the literature,
to consider a single observation $x$ in \eqref{eq:expofam-sampledens} only,
as the conjugacy property does not depend on the sample size.
However, we find our version with $n$-dimensional i.i.d.\ sample $\vec{x}$
more appropriate.} %, as `data is plural' (kaplan?).}
With the Binomial distribution, we did so indirectly only:
The $\bin(n,p)$ distribution for $S$ results from $n$ independent trials with sucess probability $p$ each.
Encoding success as $x_i=1$ and failure as $x_i=0$ and collecting the $n$ results in a vector $\vec{x}$,
we get $s =\sum_{i=1}^n x_i$.
It turns out that the sample distribution depends on $\vec{x}$ only through $s$:
\begin{align}
f(\vec{x} \mid p) &= P(\vec{X} = \vec{x}) = \prod_{i=1}^n p^{x_i} (1-p)^{1-x_i} = p^s (1-p)^{n-s}\,,
\end{align}
so $s$ summarizes the sample $\vec{x}$ without changing the pmf.
Such a summary function of a sample $\vec{x}$ is called \emph{a sufficient statistic of $\vec{x}$}.%
\footnote{There are $\binom{n}{s}$ 0/1 vectors $\vec{x}$ with $s$ 1's, leading to the Binomial pmf \eqref{eq:binompmf}.
For a Bayesian analysis, such factors that do not depend on the parameter of interest do not matter.
This is one of the central distinctions between Bayesian and Frequentist methods.}

A sample distribution is said to belong to the \emph{canonical exponential family}
if its density or mass function satisfies the decomposition
\begin{align}
f(\vec{x} \mid \theta) &= a(\vec{x})\exp\big\{\langle \psi, \tau(\vec{x}) \rangle - n \mbf{b}(\psi)\big\}\,.
\label{eq:expofam-sampledens}
\end{align}
The ingredients of the decomposition are:
\begin{itemize}
\item $\psi \in \Psi \subset \reals^q$, a transformation of the (possibly vectorial) parameter $\theta \in \Theta$,
called the \emph{natural parameter} of the canonical exponential family;
\item $\mbf{b}(\psi)$, a scalar function of $\psi$ (or, in turn, of $\theta$);
\item $a(\vec{x})$, a scalar function of $\vec{x}$;
\item $\tau(\vec{x})$, a sufficient statistic of the sample $\vec{x}$ which has the same dimension as $\psi$.
It holds that $\tau(\vec{x}) = \sum_{i=1}^n \tau^*(x_i)$,
where $\tau^*(x_i) \in \mathcal{T} \subset \reals^q$.
\item $\langle\cdot, \cdot\rangle$ denotes the scalar product,
i.e., for $u, v \in \reals^q$ is $\langle u, v\rangle = \sum_{j=1}^q u_j \cdot v_j$.
\end{itemize}
In this exercise, we will only consider one-parametric exponential family distributions,
such that $q=1$ and $\langle \psi, \tau(\vec{x}) \rangle = \psi \cdot \tau(\vec{x})$.

From these ingredients, a conjugate prior on $\psi$ can be constructed as
\begin{align}
p(\psi \mid \nz, \yz) \dd\psi
 &\propto \exp\Big\{ \nz \Big[ \langle \yz, \psi \rangle - \mbf{b}(\psi) \Big] \Big\} \dd\psi\,,
\label{eq:canonicalprior}
\end{align}
where $\nz > 0$ and $\yz \in \mathcal{T}$ are the parameters by which a certain prior can be specified.%
\footnote{Actually, the domain of $\yz$ is $\mathcal{Y}$, defined as the interior of the convex hull of $\mathcal{T}$;
these intricacies do not matter in this exercise however.}
We will refer to priors of the form~\eqref{eq:canonicalprior} as \emph{canonically constructed priors}.
Note that \eqref{eq:canonicalprior} is a distribution over the natural parameter $\psi$ and not over the usual parameter $\theta$.
When $\psi \neq \theta$ it can be useful to transform the density over $\psi$ to a density over $\theta$.

The prior parameters $\nz$ and $\yz$ are updated to their posterior values $\nn$ and $\yn$ by
\begin{align}\label{eq:canonicalupdate}
\nn &= \nz + n\,, &
\yn &= \frac{\nz}{\nz + n} \cdot \yz + \frac{n}{\nz + n} \cdot \frac{\tau(\vec{x})}{n}\,,
\end{align}
and the posterior can be written as
\begin{align}\label{eq:canonicalposterior}
p(\psi \mid \vec{x}, \nz, \yz)
 &= p(\psi \mid \nn, \yn) \nonumber\\
 &\propto \exp\Big\{ \nn \Big[ \langle \yn, \psi \rangle - \mbf{b}(\psi) \Big] \Big\} \dd\psi\,.
\end{align}
In this setting, $\yz$ and $\yn$ can be seen as the parameter describing the main characteristics of the prior and the posterior,
and thus we will call them \emph{main prior} and \emph{main posterior parameter}, respectively.
$\yz$ can also be understood as a prior guess for the mean sufficient statistic $\ttau(\vec{x}) := \tau(\vec{x})/n$.%
\footnote{This is because $\E[\ttau(\vec{x}) \mid \psi] = \nabla\mbf{b}(\psi)$,
where in turn $\E[\nabla\mbf{b}(\psi) \mid \nz, \yz] = \yz$, see \textcite[Prop.~5.7, p.~275]{2000:bernardosmith}.
Nevermind.}
Here, $\yn$ is a weighted average of this prior guess $\yz$ and the sample `mean' $\ttau(\vec{x})$,
with weights $\nz$ and $n$, respectively.
Again, $\nz$ can be seen as ``prior strength'' or ``pseudocount'',
reflecting the weight one gives to the prior as compared to the sample size $n$.

\begin{myex}
Confirm \eqref{eq:canonicalupdate},
the equations for updating $\nz$ to $\nn$ and $\yz$ to $\yn$,
for one-parametric exponential family distributions.
(Hint: Use the last expression in \eqref{eq:bayesrule}
and consider only the terms related to $\psi$ for the posterior.)
\end{myex}


\section{Some canonically constructed conjugate priors}

This looks all suspiciously similar to the reparametrised Beta distribution above.
Indeed, the $\be(\nz,\yz)$ is the canonically constructed prior to the Binomial distribution:
The pmf \eqref{eq:binompmf} can be rewritten as follows:
\begin{align}
f(s\mid p)
 &= {n \choose s} p^s (1-p)^{n-s} \\
 &= {n \choose s} \exp\Big\{ \log\Big(\frac{p}{1-p}\Big) s - n \big(-\log(1-p)\big) \Big\} \,.
%\label{}
\end{align}
We have thus $\psi = \log(p/(1-p))$, $\mbf{b}(\psi) = -\log(1-p)$, and $\tau(\vec{x}) = s$.
The function $\log(p/(1-p))$ is known as the \emph{logit}, denoted by $\logit(p)$.
Constructing the prior from these ingredients leads to
\begin{align}
f\big(\psi \mid \nz, \yz\big) \dd\psi %\dd\log\big(\frac{\theta}{1-\theta}\big)
 &\propto \exp\Big\{ \nz \Big[ \yz \log\Big(\frac{p}{1-p}\Big) + \log(1-p) \Big] \Big\} \dd\psi\,.
 %\dd\log\big(\frac{\theta}{1-\theta}\big)\,.
\end{align}
To transform this density over $\psi$ to a density over $p$,
we have to multiply it with
\begin{align}
\left|\frac{\dd\psi}{\dd p}\right|
 &= \left|\frac{\dd}{\dd p} \log\Big(\frac{p}{1-p}\Big)\right|
  = \left| \frac{1-p}{p}\left(\frac{(1-p)+p}{(1-p)^2}\right) \right|
  = \frac{1}{p(1-p)}\,,
\end{align}
and so we get
\begin{align}
\lefteqn{f(p\mid \nz, \yz)\dd p} \qquad \\  
 &= f(\psi\mid \nz, \yz) \left|\frac{\dd\psi}{\dd p}\right| \dd p\\
 &\propto \exp\Big\{ \nz\yz\log(p) + \big(\nz - \nz\yz\log(1-p)\big)\Big\} \frac{1}{p(1-p)}\dd p\\
 &= p^{\nz\yz - 1} (1-p)^{\nz(1-\yz) - 1} \dd p\,.
\end{align}
This is indeed the Beta distribution \eqref{eq:betadensny}.%with parameters $\nz\yz$ and $\nz(1-\yz)$.

\begin{myex}
Construct the canonical conjugate prior to a sample distribution of your choice.
This works only for distributions forming a canonical exponential family!
(As a counterexample, the Weibull distribution with unknown shape parameter does not form a canonical exponential family.)
You can try, e.g., the Normal (Gaussian) distribution with fixed variance $\sigma^2_0$
or, if you want to avoid a density transformation, the Normal distribution with fixed variance~$1$.%
\footnote{You can find the solution for $\vec{X} \stackrel{\text{i.i.d}}{\sim} \norm(\mu, \sigma^2_0)$
at \textcite[pp.~14f]{2013:diss-gw}.
Table~1 in \textcite{2005:quaeghebeurcooman} gives $\psi$, $\tau^*(x_i)$, $a(x)$ and $\mbf{b}(\psi)$
for the most common sample distributions that form a canonical exponential family.}
\end{myex}


\section{Prior-data conflict}

As discussed in the talk, the weighted average structure for $\yn$ in \eqref{eq:canonicalupdate} is intuitive,
but with it comes the problem of prior-data conflict.
In most parametric models, the spread of the posterior does not systematically depend
on how far the prior guess $\yz$ diverges from the mean sufficient statistic $\tilde{\tau}(\vec{x})$.
Then, conflict between prior assumptions and information from data is just averaged out,
and the posterior gives a false impression of certainty,
being more pointed around $\yn$ than the prior in spite of the conflict.

\begin{myex}
Write the functions \code{nn(n0, n)} and \code{yn(n0, y0, s, n)}
implementing \eqref{eq:betapostny}, the update step for the Beta prior.
Plot prior and posterior densities for different choices of \code{n0, y0, s, n}
to see the effect (or the lack of it) of prior-data conflict on the posterior,
using your code from Exercise~\ref{ex:dbetany}.
E.g, take \code{n0 = 8}, \code{y0 = 0.75} to fix a prior,
and compare the posterior for \code{s = 12, n = 16} and for \code{s = 0, n = 16}.
\end{myex}


\section{Sets of conjugate priors}

Modelling prior information with sets of conjugate priors
retains the tractability of conjugate analysis and gives prior-data conflict sensitivity.
It also allows to express prior knowledge more cautiously, or partial prior knowledge,
and generally makes it possible to express the precision of probability statements encoded in the prior.
The resulting imprecise/interval probability models can be seen as systematic sensitivity analysis,
or as a kind of robust Bayesian method.

The central idea is to consider sets $\PZ$ of canonical parameters $(\nz, \yz)$
which create corresponding sets of priors $\MZ$ via
\begin{align}
\MZ = \big\{f(\psi\mid\nz,\yz) : (\nz, \yz) \in \PZ\big\}\,.
\end{align}
\textcite{2005:quaeghebeurcooman} suggested sets $\PZ = \nz \times [\yzl,\yzu]$,
but \textcite{2009:WalterAugustin} showed that the resulting sets of priors are still insensitive to prior-data conflict,
and proposed instead to use sets $\PZ = [\nzl, \nzu] \times [\yzl,\yzu]$.%
\footnote{A detailed discussion of different types of $\PZ$ is given in \textcite[\S 3.1]{2013:diss-gw}.}
%\footnote{You can have a paper copy if you ask me!}.
Sets of priors generated by $\PZ = [\nzl, \nzu] \times [\yzl,\yzu]$
were called `generalized iLUCK-models',
and were implemented in an \textbf{R} package \texttt{luck}
\parencite{luck-package}.

\begin{myex}
You can install the \code{luck} package 
by downloading \emph{\url{http://download.r-forge.r-project.org/src/contrib/luck_0.9.tar.gz}}
to your working directory and then executing\\
\verb+install.packages("luck_0.9.tar.gz", repos = NULL, type = "source")+
%by executing\\
%\verb+install.packages("http://download.r-forge.r-project.org/src/contrib/luck_0.9.tar.gz", repos = NULL, type = "source")+
at the \textbf{R} prompt.
If there is a problem, try installing the package \code{TeachingDemos} first.
After installation, load the package by executing \code{library(luck)}.
\begin{enumerate}[(i)]
\item Use the functions \code{LuckModel()} and \code{LuckModelData()}
to create a \code{LuckModel} object corresponding to a parameter set $\PZ$ with an interval for both $\nz$ and $\yz$,
and that contains data such that $\tau(\vec{x})/n \in [\yzl, \yzu]$.
Create a second \code{LuckModel} object with the same $\PZ$ but for which $\tau(\vec{x})/n \not\in [\yzl, \yzu]$.
\item Plot the prior and posterior parameter sets $\PZ$ and $\PN$ for both objects.
You can find the help for the plot function for \code{LuckModel} objects via \code{?luck::plot}.
To plot $\PN$, you need to supply the option \code{control=controlList(posterior=TRUE)};
to plot a second parameter set in the same plot window, use \code{add=TRUE}.
(You may need to set \code{xlim} and \code{ylim} to make the plotting region large enough!)
\item Can you explain why the two $\PN$'s have different shapes,
and how their respective shapes come about?
(Hint: Each point in the upper bound of $\PN$ is a weighted average of $\yzu$ and $\tau(\vec{x})/n$.)
\item Vary the $\yz$ interval, the $\nz$ interval, the sample statistic $\tau(\vec{x})$, the sample size $n$,
and observe the effect each has on $\PN$.
What happens to $\PZ$ when $\nz \to \infty$ with $\ttau(\vec{x}) = \tau(\vec{x})/n$ constant?
%(sets shrink towards $\ttau(\vec{x})$ for $n \to \infty$, \dots, as in \parencite[\S 3.1.2]{2013:diss-gw}).
\end{enumerate}
\end{myex}

A model with $\PZ = \nz \times [\yzl, \yzu]$ corresponds to a vertical slice of the plotted sets $\PZ$.
The posterior parameter set is a vertical slice as well and can be expressed as $\PN = \nn \times [\ynl, \ynu]$,
where $\ynl$ and $\ynu$ result from updating $\yzl$ and $\yzu$, respectively:
\begin{align}
\ynl &= \frac{\nz\yzl + \tau(\vec{x})}{\nz + n}\,, &
\ynu &= \frac{\nz\yzu + \tau(\vec{x})}{\nz + n}\,.
\end{align}
The posterior imprecision in the $y$ dimension, denoted by $\Delta_y(\PN)$, is 
\begin{align}
\label{eq:deltay}
\Delta_y(\PN) &= \ynu - \ynl = \frac{\nz (\yzu - \yzl)}{\nz +n}\,.
\end{align}
\begin{myex}
Do you see from \eqref{eq:deltay} why models with $\PZ = \nz \times [\yzl, \yzu]$ are insensitive to prior-data conflict?
\end{myex}
%***$\ynl$, $\ynu$ formulas. $\ynu-\ynl$ formula? \parencite[p.~65]{2013:diss-gw}
When $\PZ = [\nzl, \nzu] \times [\yzl, \yzu]$, things are different, as you have seen.
Then, the lower and upper bound in the $y$ dimension are given by
\begin{align}
\label{eq:ynl}
\ynl &= \inf_{\PN} \yn = \begin{cases} \dfrac{\nzu}{\nzu + n}\,\yzl + \dfrac{n}{\nzu + n}\,\ttaux & \ttaux \geq \yzl \\[2ex]
                                       \dfrac{\nzl}{\nzl + n}\,\yzl + \dfrac{n}{\nzl + n}\,\ttaux & \ttaux <    \yzl \end{cases}\,,\\
\label{eq:ynu}
\ynu &= \sup_{\PN} \yn = \begin{cases} \dfrac{\nzu}{\nzu + n}\,\yzu + \dfrac{n}{\nzu + n}\,\ttaux & \ttaux \leq \yzu \\[2ex]
                                       \dfrac{\nzl}{\nzl + n}\,\yzu + \dfrac{n}{\nzl + n}\,\ttaux & \ttaux >    \yzu \end{cases}\,.
\end{align}
\begin{myex}
Which cases in \eqref{eq:ynl} and \eqref{eq:ynu} correspond to prior-data conflict?
What do they have in common, and how does this link to what you saw in the $\PN$ plots?
\end{myex}
The posterior imprecision in the $y$ dimension can be expressed by
\begin{align}
\Delta_y(\PN) &= \frac{\nzu (\yzu - \yzl)}{\nzu + n}\\
              &\qquad + \inf_{\yz \in [\yzl,\yzu]} |\ttaux - \yz| \frac{n (\nzu - \nzl)}{(\nzl + n)(\nzu + n)}\,.
\end{align}
Note that the expression $\inf_{\yz \in [\yzl,\yzu]} |\ttaux - \yz| = 0$ when $\ttaux \in [\yzl, \yzu]$,
otherwise it gives the distance of $\ttaux$ to the $\yz$ interval.


\section{Sets of conjugate priors for data $X \sim \norm(\mu,1)$}

***plot sets of prior/posterior cdfs, union of HPD intervals.

***write own subclasses, make own analyses.




\printbibliography

\end{document}
