\documentclass[12pt,a4paper	,twoside]{article}

% ------------ packages -------------

\usepackage[utf8]{inputenc}
\usepackage[OT1]{fontenc}
\usepackage{graphicx}
\usepackage[english]{babel}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{todonotes}
\usepackage{etoolbox}
\usepackage{url}
%\usepackage{tikz}
%\usetikzlibrary{shapes.misc,fit}

\usepackage[bookmarks]{hyperref}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}
\newcommand{\posrealszero}{\reals_{\ge 0}}
\newcommand{\naturals}{\mathbb{N}}

\newcommand{\dd}{\,\mathrm{d}}

\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\renewcommand{\vec}[1]{{\bs#1}}

\newcommand{\uz}{^{(0)}} % upper zero
\newcommand{\un}{^{(n)}} % upper n
\newcommand{\ui}{^{(i)}} % upper i

\newcommand{\ul}[1]{\underline{#1}}
\newcommand{\ol}[1]{\overline{#1}}

\newcommand{\E}{\operatorname{E}}
\newcommand{\V}{\operatorname{Var}}

\newcommand{\ber}{\operatorname{Bernoulli}} 
\newcommand{\bin}{\operatorname{Binomial}}
\newcommand{\be}{\operatorname{Beta}} 
\newcommand{\bebin}{\operatorname{Beta-binomial}} 

\newtheorem{myex}{Exercise}

\input{nydefs.tex}

\title{Generalized Bayesian Inference\\ with Sets of Conjugate Priors}
\author{Gero Walter}
\date{Lund University, 15.12.2015}

\begin{document}
\maketitle

\section{Bayesian basics}

Bayesian inference allows to combine information from data
and information extraneous to data (e.g., expert information)
into a `complete picture'.
Data $X$ is assumed to be generated from a certain parametric distribution family,
e.g., a Binomial distribution with parameters $p$ and $n$,
where $n$ is known but $p \in [0,1]$ is unknown.
In short, $X\mid p \sim \bin(n,p)$, which means
\begin{align*}
f(x\mid p) &= P(X = x \mid p) = {n \choose x} p^x (1-p)^{n-x},\quad x \in \{0, 1, \ldots, n\}\,.
\end{align*}
Information about unknown parameters (here, $p$) is then expressed
by a so-called prior distribution, some distribution with some pdf, here $f(p)$.

The `complete picture' is then the so-called posterior distribution,
here with pdf $f(p\mid x)$, expressing the state of knowledge after having seen the data.
It encompasses information from the prior $f(p)$ and data
and is obtained via Bayes' Rule,
\begin{align}
f(p \mid x) &= \frac{f(x\mid p) f(p)}{\int f(x\mid p) f(p) \dd p}
             = \frac{f(x\mid p) f(p)}{f(x)} \propto f(x\mid p) f(p)\,,
\label{eq:bayesrule}
\end{align}
where $f(x)$ is the so-called marginal distribution of the data $X$.

In general, the posterior distribution is hard to obtain,
especially due to the integral in the denominator.
The posterior can be approximated with numerical methods,
like the Laplace approximation or simulation methods like MCMC (Markov chain Monte Carlo).
There is a large literature dealing with computations of posteriors,
and software like BUGS or JAGS has been developed
which simplifies the creation of a sampler to approximate a posterior.


\section{Conjugate priors}

However, Bayesian inference not necessarily entails complex calculations and simulation methods.
With a clever choice of parametric family for the prior distribution,
the posterior distribution belongs to the same parametric family as the prior,
just with updated parameters.
Such prior distributions are called \emph{conjugate} priors.
Basically, with conjugate priors one trades flexibility for tractability:
The parametric family restricts the form of the prior,
but with the advantage of much easier computations.

The conjugate prior for the Binomial distribution is the Beta distribution,
which is usually parametrised with parameters $\alpha$ and $\beta$.
\begin{align}
f(p\mid\alpha,\beta) &= \frac{1}{B(\alpha,\beta)}\, p^{\alpha-1}\, (1-p)^{\beta-1}\,,
\label{eq:betadensab}
\end{align}
where $B(\cdot,\cdot)$ is the Beta function.%
\footnote{The Beta function is defined as $B(a,b) = \int_0^1 t^{a-1} (1-t)^{b-1} \dd t$
and gives the inverse normalisation constant for the Beta distribution.
It is related to the Gamma function through $B(a,b) = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$.
We will not need to work with Beta functions here.}
In short, we write $p \sim \beta(\alpha,\beta)$.

From now on, we will denote prior parameter values by an upper index ${}\uz$,
and updated, posterior parameter values by an upper index ${}\un$.

With this notational convention,
let $X\mid p \sim \bin(n,p)$ and $p \sim \beta(\az,\bz)$.
Then it holds that $p \mid x \sim \be(\an,\bn)$,
where $\an$ and $\bn$ are updated, posterior parameters, obtained as
\begin{align}
\an &= \az + x\,, & \bn &= \bz + n - x \,.
\label{eq:betapostab}
\end{align}
***Interpretation***

\begin{myex}
Confirm \eqref{eq:betapostab}, i.e.,
show that, when $X\mid p \sim \bin(n,p)$ and $p \sim \beta(\az,\bz)$,
the density of the posterior distribution for $p$ is of the form \eqref{eq:betadensab}
but with updated parameters.
(Hint: use the last expression in \eqref{eq:bayesrule}
and consider in the posterior only the terms related to $p$.)
\end{myex}

You have seen in the talk that we considered a different parametrisation of the Beta distribution
in terms of $\nz$ and $\yz$, obtained as
\begin{align}
\nz &= \az + \bz\,, & \yz &= \frac{\az}{\az+\bz}\,,
\label{eq:betareparam}
\end{align}
such that writing $p \sim \be(\nz,\yz)$ corresponds to
\begin{align}
f(p\mid\nz,\yz) &= \frac{p^{\nz\yz-1}\, (1-p)^{\nz(1-\yz)-1}}{B(\nz\yz,\nz(1-\yz))}\,.
\label{eq:betadensny}
\end{align}
In this parametrisation, the updated, posterior parameters are given by
\begin{align}
\nn &= \nz + n\,, &
\yn &= \frac{\nz}{\nz + n} \cdot \yz + \frac{n}{\nz + n} \cdot \frac{x}{n}\,,
\label{eq:betapostny}
\end{align}
and so we write $p\mid x \sim \be(\nn,\yn)$.

\begin{myex}
Confirm the equations for updating $\nz$ to $\nn$ and $\yz$ to $\yn$,
starting either from \eqref{eq:betapostab} or \eqref{eq:betadensny}.
\end{myex}

From the properties of the Beta distribution,
it follows that $\yz = \frac{\az}{\az+\bz} = \E[p]$
is the prior expectation for the success probability $p$,
and that the higher $\nz$, the more probability weight will be concentrated around $\yz$,
as $\V(p) = \frac{\yz (1-\yz)}{\nz + 1}$.

\begin{myex}
Write a function \texttt{dbetany(x,n,y, ...)}
that returns the value of the Beta density function at \texttt{x}
for parameters $\nz$, $\yz$ instead of \texttt{shape1} ($=\alpha$) and \texttt{shape2} ($=\beta$)
as in \texttt{dbeta(x, shape1, shape2, ...)}.
Use your function to plot the Beta density for different parameter values
to see how the density changes according to the parameter values.
\end{myex}

$\nz$ can also be interpreted as a pseudocount or prior strength,
(do you see how this is clear from the interpretation of $\alpha$ and $\beta$ and \eqref{eq:betareparam}?)

***Fomula for $\yn$ in \eqref{eq:betapostny} not written in the most compact form
to emphasize thet the $\yz$ update has a peculiar form:***

so after seeing that $s$ out of $n$ components function (at time $t$),
the posterior mean $\yn$ for $p$ is a weighted average of
the prior mean $\yz$ and $s/n$ (the fraction of functioning components in the data),
with the weights $\nz$ and $n$, respectively.
We see that $\nz$ plays the same role for the prior mean $\yz$
as the sample size $n$ for the observed mean $s/n$,
thus the notion of pseudocount.
Indeed, the higher $\nz$, the higher the weight for $\yz$
in the weighted average calculation of $\yn$,
so $\nz$ gives the strength of the prior as compared to the sample size $n$.



\end{document}
